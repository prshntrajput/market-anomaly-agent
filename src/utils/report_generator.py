"""Structured Investigation Report Generator"""

from datetime import datetime
from src.models.schemas import StockAnomaly
from src.chains.evidence_evaluator import EvidenceEvaluation
from typing import Dict


class InvestigationReportGenerator:
    """
    Generate professional investigation reports
    """
    
    def generate_markdown_report(
        self,
        anomaly: StockAnomaly,
        evaluation: EvidenceEvaluation,
        queries_used: list[str],
        iterations: int
    ) -> str:
        """
        Generate comprehensive markdown report
        """
        
        report = f"""# Market Anomaly Investigation Report

**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Investigation ID**: INV-{datetime.now().strftime('%Y%m%d-%H%M%S')}

---

## ðŸš¨ Anomaly Summary

**Stock**: {anomaly.ticker}
**Price Change**: {anomaly.price_change_percent:+.2f}%
**Current Price**: ${anomaly.price:.2f}
**Volume Spike**: {anomaly.volume_ratio:.1f}x normal
**Severity**: {"ðŸ”´ CRITICAL" if abs(anomaly.price_change_percent) > 15 else "ðŸŸ¡ HIGH"}
**Timestamp**: {anomaly.timestamp.strftime('%Y-%m-%d %H:%M:%S')}

---

## ðŸ” Investigation Findings

### Root Cause Analysis

**Status**: {"âœ… IDENTIFIED" if evaluation.explanation_found else "âš ï¸ INCONCLUSIVE"}
**Confidence**: {evaluation.confidence:.0%}
**Quality**: {evaluation.explanation_quality.upper()}

**Explanation**:
{evaluation.root_cause}

**Reasoning**:
{evaluation.reasoning}

---

## ðŸ“Š Evidence Quality Metrics

| Metric | Score | Assessment |
|--------|-------|------------|
| Source Credibility | {evaluation.overall_credibility:.0%} | {self._assess_score(evaluation.overall_credibility)} |
| Content Relevance | {evaluation.overall_relevance:.0%} | {self._assess_score(evaluation.overall_relevance)} |
| Overall Confidence | {evaluation.confidence:.0%} | {self._assess_score(evaluation.confidence)} |

---

## ðŸ”Ž Evidence Details

**Total Evidence Items**: {len(evaluation.evidence_items)}

### Top Evidence (by quality)

"""
        
        # Add top 5 evidence items
        sorted_evidence = sorted(
            evaluation.evidence_items,
            key=lambda x: x.relevance_score * x.source_credibility,
            reverse=True
        )[:5]
        
        for i, item in enumerate(sorted_evidence, 1):
            report += f"""
#### Evidence {i}
- **Source**: {item.source_url}
- **Credibility**: {item.source_credibility:.0%}
- **Relevance**: {item.relevance_score:.0%}
- **Content**: {item.content[:200]}...

"""
        
        # Add missing information
        if evaluation.missing_info:
            report += f"""---

## âš ï¸ Information Gaps

The following information would improve investigation confidence:

"""
            for info in evaluation.missing_info:
                report += f"- {info}\n"
        
        # Add investigation metadata
        report += f"""
---

## ðŸ“ˆ Investigation Metadata

**Iterations**: {iterations}
**Queries Executed**: {len(queries_used)}

### Search Queries Used

"""
        for i, query in enumerate(queries_used, 1):
            report += f"{i}. `{query}`\n"
        
        report += f"""
---

## ðŸŽ¯ Recommendations

"""
        # Generate recommendations based on confidence
        if evaluation.confidence >= 0.8:
            report += """
âœ… **High Confidence**: Root cause identified with credible sources.
- Action: Monitor for follow-up developments
- Alert Level: Informational
"""
        elif evaluation.confidence >= 0.6:
            report += """
âš ï¸ **Medium Confidence**: Likely cause identified but needs verification.
- Action: Continue monitoring and verify with additional sources
- Alert Level: Watch
"""
        else:
            report += """
ðŸ”´ **Low Confidence**: Unable to determine root cause conclusively.
- Action: Manual investigation recommended
- Alert Level: Review Required
"""
        
        report += """
---

*Generated by Market Anomaly Detection Agent v1.0*
"""
        
        return report
    
    def _assess_score(self, score: float) -> str:
        """Assess score quality"""
        if score >= 0.8:
            return "âœ… Excellent"
        elif score >= 0.6:
            return "âœ”ï¸ Good"
        elif score >= 0.4:
            return "âš ï¸ Fair"
        else:
            return "âŒ Poor"
    
    def save_report(
        self,
        report: str,
        ticker: str,
        filepath: str = None
    ) -> str:
        """Save report to file"""
        if filepath is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filepath = f"logs/investigation_{ticker}_{timestamp}.md"
        
        with open(filepath, 'w') as f:
            f.write(report)
        
        return filepath


# Test function
def test_report_generator():
    """Test report generation"""
    
    print("\n" + "="*60)
    print("ðŸ§ª TESTING REPORT GENERATOR")
    print("="*60)
    
    from src.chains.evidence_evaluator import EvidenceItem, EvidenceEvaluation
    
    # Mock data
    anomaly = StockAnomaly(
        ticker="TSLA",
        price=426.48,
        price_change_percent=-16.2,
        volume=95000000,
        volume_ratio=6.1
    )
    
    evaluation = EvidenceEvaluation(
        evidence_items=[
            EvidenceItem(
                content="Tesla Q3 2025 earnings missed expectations significantly",
                source_url="https://www.bloomberg.com/news/tesla",
                source_credibility=0.94,
                relevance_score=0.92,
                specificity_score=0.88
            )
        ],
        overall_credibility=0.90,
        overall_relevance=0.85,
        explanation_found=True,
        explanation_quality="excellent",
        root_cause="Q3 2025 earnings miss with 20% revenue decline and lowered guidance",
        confidence=0.88,
        reasoning="Multiple credible sources confirm earnings miss as primary driver",
        missing_info=["Detailed analyst commentary", "Management guidance specifics"]
    )
    
    generator = InvestigationReportGenerator()
    
    report = generator.generate_markdown_report(
        anomaly=anomaly,
        evaluation=evaluation,
        queries_used=["TSLA earnings Q3 2025", "Tesla revenue guidance cut"],
        iterations=2
    )
    
    print("\nðŸ“„ Generated Report:\n")
    print(report[:1000] + "...")
    
    # Save report
    filepath = generator.save_report(report, "TSLA")
    print(f"\nâœ… Report saved to: {filepath}")
    
    print("\n" + "="*60)


if __name__ == "__main__":
    test_report_generator()
